{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Research question** : one could explore how the popularity of specific songs on the \"mymp3song.site\" platform, as indicated by user downloads, correlates with social media trends and discussions. To answer this question, data should be collected from relevant social media platforms where users discuss and share information about music. APIs for platforms like Twitter, Instagram, or Facebook could be utilized to gather user-generated content, including posts, comments, and interactions related to the songs available on \"mymp3song.site.\" The collected data should include information on sentiment, user engagement metrics, and possibly demographic details of the users participating in discussions. Additionally, temporal analysis could be conducted to observe how social media trends evolve over time and whether they coincide with fluctuations in the popularity of specific songs. The amount of data needed would depend on the frequency of discussions and the desired granularity of analysis, but a substantial dataset covering several months could provide more meaningful insights. The data collected should be stored in a structured format, maintaining separate tables for posts, comments, engagement metrics, and song details. It's crucial to ensure compliance with privacy laws and document the data collection methodology for transparency and reproducibility. By analyzing this integrated dataset, researchers could uncover patterns linking social media trends to user engagement and downloads on the music platform, shedding light on the influence of online discussions on consumer behavior in the music industry."
      ],
      "metadata": {
        "id": "J3WUQWVuXKnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_2vspm32Z5Er"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import bs4\n",
        "import os\n",
        "\n",
        "BASE_URL = 'http://mymp3song.site'\n",
        "INDEX_FILE = 'index.txt'\n",
        "SONGS_DIR = 'Songs'\n",
        "\n",
        "\n",
        "def add_in_file(title):\n",
        "    try:\n",
        "        with open(INDEX_FILE, 'a') as index:\n",
        "            index.seek(0)\n",
        "            index.writelines(title + \"\\n\")\n",
        "            print(\"Title Added Successfully\")\n",
        "    except Exception as e:\n",
        "        print(\"Error in index adding: \", e)\n",
        "\n",
        "\n",
        "def search_in_file(title):\n",
        "    try:\n",
        "        with open(os.path.join(SONGS_DIR, INDEX_FILE), 'r') as index:\n",
        "            return title in index.read()\n",
        "    except Exception as e:\n",
        "        print(\"Error in index searching : \", e)\n",
        "        return False\n",
        "\n",
        "\n",
        "def song_download(url, title):\n",
        "    if not os.path.exists(SONGS_DIR):\n",
        "        os.mkdir(SONGS_DIR)\n",
        "    os.chdir(SONGS_DIR)\n",
        "\n",
        "    try:\n",
        "        with requests.get(url, stream=True) as req:\n",
        "            req.raise_for_status()\n",
        "            with open(title + \".mp3\", \"wb\") as f:\n",
        "                f.write(req.content)\n",
        "                print(\"Downloaded Successfully\")\n",
        "                add_in_file(title)\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(\"Error in download:\", e)\n",
        "    finally:\n",
        "        os.chdir('..')\n",
        "\n",
        "\n",
        "# Example: Download a specific song\n",
        "song_url = 'http://mymp3song.site/download/41578/channa-ve'\n",
        "song_title = 'Channa Ve'\n",
        "\n",
        "song_download(song_url, song_title)"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60651a0c-e5e4-493f-92b0-b20ebd9ebba7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded Successfully\n",
            "Title Added Successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaGLbSHHB8Ej"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import time\n",
        "\n",
        "def scrape_articles(query, num_articles):\n",
        "    base_url = \"https://scholar.google.com/scholar\"\n",
        "    articles = []\n",
        "\n",
        "    for page in range(0, num_articles, 10):\n",
        "        params = {\n",
        "            \"start\": page,\n",
        "            \"q\": query,\n",
        "            \"hl\": \"en\",\n",
        "            \"as_sdt\": \"0,5\",\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(base_url, params=params)\n",
        "            response.raise_for_status()  # Check for HTTP errors\n",
        "\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "            results = soup.find_all(\"div\", class_=\"gs_ri\")\n",
        "\n",
        "            for result in results:\n",
        "                title = result.find(\"h3\", class_=\"gs_rt\").text\n",
        "                authors = result.find(\"div\", class_=\"gs_a\").text\n",
        "                link = result.find(\"a\")[\"href\"]\n",
        "\n",
        "                # Fetch additional details from the article page\n",
        "                article_page_response = requests.get(link)\n",
        "                article_soup = BeautifulSoup(article_page_response.text, \"html.parser\")\n",
        "\n",
        "                venue_element = article_soup.find(\"div\", class_=\"gs_scl\")\n",
        "                venue = venue_element.text.strip() if venue_element else \"N/A\"\n",
        "\n",
        "                year_element = article_soup.find(\"span\", class_=\"gs_ri\")\n",
        "                year = year_element.text.split(\",\")[-1].strip() if year_element else \"N/A\"\n",
        "\n",
        "                abstract_element = article_soup.find(\"div\", class_=\"gsh_small\")\n",
        "                abstract = abstract_element.text.strip() if abstract_element else \"N/A\"\n",
        "\n",
        "                article_info = {\n",
        "                    \"Title\": title,\n",
        "                    \"Authors\": authors,\n",
        "                    \"Link\": link,\n",
        "                    \"Venue\": venue,\n",
        "                    \"Year\": year,\n",
        "                    \"Abstract\": abstract,\n",
        "                }\n",
        "                articles.append(article_info)\n",
        "\n",
        "                # Pause to avoid rate-limiting\n",
        "                time.sleep(1)\n",
        "\n",
        "        except (requests.RequestException, ConnectionError) as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            continue  # Continue with the next iteration\n",
        "\n",
        "    return articles\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    keyword = \"Web scrapping\"\n",
        "    num_articles = 1000\n",
        "    articles = scrape_articles(keyword, num_articles)\n",
        "\n",
        "    # Print the first article as a sample\n",
        "    print(json.dumps(articles[0], indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FCdCiXmPpVws"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I cannot directly access or interact with external tools, online services, or shared storage. However, I can guide you on how to use a web scraping tool like ParseHub or Octoparse."
      ],
      "metadata": {
        "id": "JFNEgDmSpne_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**# Mandatory Questionitalicized text"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ku-SnY1Sp9fi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SrbkOpGOp-As"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FBKvD6O_TY6e",
        "E9RqrlwdTfvl",
        "03jb4GZsBkBS",
        "sZOhks1dXWEe"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}